{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. What is a random variable in probability theory?"
      ],
      "metadata": {
        "id": "11NlZJD5lZBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In probability theory, a random variable is a function that assigns a numerical value to each outcome in a sample space of a random experiment.\n",
        "\n",
        "There are two main types of random variables:\n",
        "\n",
        "1. Discrete random variable ‚Äì Takes on a countable number of distinct values.\n",
        "Example: The number of heads in 3 coin tosses (possible values: 0, 1, 2, 3).\n",
        "\n",
        "2. Continuous random variable ‚Äì Takes on an infinite number of possible values within a range.\n",
        "Example: The exact height of a randomly chosen person."
      ],
      "metadata": {
        "id": "nHG1ACmClxFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. What are the types of random variables?"
      ],
      "metadata": {
        "id": "jFLVy5Xql7uZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Random variables in probability theory are broadly classified into two main types: discrete and continuous. A discrete random variable is one that can take on a countable number of distinct values. These values are typically whole numbers, such as the result of rolling a die (1 through 6) or the number of heads in a series of coin tosses. The probabilities associated with each possible value are given by a probability mass function (PMF), and the total of all probabilities is equal to 1.\n",
        "\n",
        "* On the other hand, a continuous random variable can take on an infinite number of possible values within a given range, often representing measurements like height, weight, or time. Since the number of potential values is uncountably infinite, the probability of the variable taking on any single exact value is zero. Instead, probabilities are calculated over intervals using a probability density function (PDF), and the total area under the curve of the PDF is equal to 1."
      ],
      "metadata": {
        "id": "rkS7nlaNl7YS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. What is the difference between discrete and continuous distributions?"
      ],
      "metadata": {
        "id": "NHgafQqwl7MJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Discrete Distributions:\n",
        "A discrete distribution describes the probability of outcomes for a discrete random variable, which takes on a countable number of distinct values. Probabilities are assigned to each individual value using a probability mass function (PMF).\n",
        "\n",
        "2. Continuous Distributions:\n",
        "   A continuous distribution describes a continuous random variable, which can take on any value within an interval (often real numbers). Probabilities are described using a probability density function (PDF), and exact values have a probability of zero.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T6SGA_Fjl6_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. What are probability distribution functions (PDF)?"
      ],
      "metadata": {
        "id": "Zu2_MCT4oIpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A probability distribution function (PDF) is a mathematical function that describes the probability of different possible outcomes of a random event. It essentially assigns a probability to each possible value of a random variable. In simpler terms, it shows how likely it is that a random variable will take on a certain value."
      ],
      "metadata": {
        "id": "Sxtr-Ue1oSnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. How do cumulative distribution functions (CDF) differ from probability distribution functions (PDF)?"
      ],
      "metadata": {
        "id": "Ryq_UATYoSji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary difference between a Cumulative Distribution Function (CDF) and a Probability Distribution Function (PDF) lies in what they represent: a CDF provides the probability of a random variable being less than or equal to a specific value, while a PDF describes the probability density at specific points. In essence, the CDF accumulates probabilities, while the PDF focuses on individual probabilities or probability densities."
      ],
      "metadata": {
        "id": "KrUmsvh8oSf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. What is a discrete uniform distribution?"
      ],
      "metadata": {
        "id": "kMKjkxpQoSdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A discrete uniform distribution is a type of discrete probability distribution in which all possible outcomes are equally likely. That means each value in the finite set of outcomes has the same probability.\n",
        "\n"
      ],
      "metadata": {
        "id": "gl8Y7Y2DoSVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. What are the key properties of a Bernoulli distribution?"
      ],
      "metadata": {
        "id": "kfbLbM7VoSRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Bernoulli distribution is a discrete probability distribution that describes the outcome of a single trial with exactly two possible outcomes: success (1) or failure (0). It is characterized by a single parameter,\n",
        "ùëù\n",
        "p, which represents the probability of success ùëã\n",
        "=\n",
        "1\n",
        ")\n",
        "=\n",
        "ùëù\n",
        "P(X=1)=p) and the probability of failure being\n",
        "1\n",
        "‚àí\n",
        "ùëù\n",
        "1‚àíp. The random variable\n",
        "ùëã\n",
        "X can only take values 0 or 1, and its probability mass function (PMF) is given\n",
        "Var(X)=p(1‚àíp). The distribution is commonly used to model binary outcomes, such as the result of a coin flip (e.g., heads = 1, tails = 0) or a success/failure event in various experiments. It serves as the foundation for more complex distributions, such as the binomial distribution, which sums multiple independent Bernoulli trials."
      ],
      "metadata": {
        "id": "Pq77DIQeoSNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. What is the binomial distribution, and how is it used in probability?"
      ],
      "metadata": {
        "id": "XSr2c02goSJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The binomial distribution is a discrete probability distribution that models the number of successes in a fixed number of independent trials, each with the same probability of success. It is widely used in probability theory and statistics to model experiments or processes where each trial has two possible outcomes (success or failure), like coin flips, quality control testing, or survey sampling."
      ],
      "metadata": {
        "id": "_KD_4TMqoSF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9.  What is the Poisson distribution and where is it applied?"
      ],
      "metadata": {
        "id": "NemKoKUtoSCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Poisson distribution is a discrete probability distribution that models the number of events occurring in a fixed interval of time, space, or any other domain where events happen independently and at a constant average rate. It is characterized by a single parameter,\n",
        "(lambda), which represents the average rate or mean number of events in the interval. The Poisson distribution is particularly useful for modeling rare or random events, such as the number of calls received at a call center within an hour or the number of accidents at a traffic intersection over a day. The probability of observing exactly\n",
        "k is the number of events. The distribution is widely applied in areas such as telecommunications, queuing theory, biology, and insurance, where the events are random, rare, and occur at a known constant rate. Its key properties include having a mean and variance both equal to it is particularly effective when modeling scenarios where events happen independently over a specified time or space."
      ],
      "metadata": {
        "id": "eUUUI6Fjr07m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. What is a continuous uniform distribution?"
      ],
      "metadata": {
        "id": "0uo_I1S-r02y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuous uniform distribution is a probability distribution where all outcomes in a given range are equally likely to occur. Unlike the discrete uniform distribution, which deals with a finite set of distinct outcomes, the continuous uniform distribution applies to a continuous range of values, and each value within the range has an equal probability of being observed"
      ],
      "metadata": {
        "id": "nFBaSl13r0yQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11. What are the characteristics of a normal distribution?"
      ],
      "metadata": {
        "id": "Te_bxaNir0t4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The normal distribution, also known as the Gaussian distribution, is one of the most widely used probability distributions in statistics and is often referred to as the \"bell curve\" because of its distinctive shape. It is a continuous probability distribution that is defined by its mean and standard deviation. Here are the key characteristics of a normal distribution:\n",
        "\n",
        "1. Symmetry:\n",
        "The normal distribution is perfectly symmetric around its mean. This means that the left and right sides of the curve are mirror images of each other.\n",
        "\n",
        "The mean, median, and mode of the distribution are all equal and located at the center of the distribution.\n",
        "\n",
        "2. Bell-Shaped Curve:\n",
        "The probability density function (PDF) of a normal distribution has a characteristic bell-shaped curve. It starts at 0 on the far left, rises to a peak at the mean, and then falls back toward 0 as it moves to the right.\n",
        "\n",
        "This shape reflects that most observations are clustered around the mean, and fewer observations are found farther away from the mean.\n",
        "\n",
        "3. Defined by Two Parameters:\n",
        "Mean (Œº): The mean determines the location of the center of the distribution. It is the point where the curve peaks.\n",
        "\n",
        "Standard Deviation (œÉ): The standard deviation measures the spread or \"width\" of the distribution. A larger standard deviation results in a wider curve, while a smaller standard deviation results in a narrower curve.\n",
        "\n",
        "Together, these two parameters define the shape and location of the normal distribution.\n",
        "\n",
        "4. 68-95-99.7 Rule (Empirical Rule):\n",
        "In a normal distribution:\n",
        "\n",
        "About 68% of the data falls within one standard deviation of the mean (\n",
        "ùúá\n",
        "‚àí\n",
        "ùúé\n",
        "Œº‚àíœÉ to\n",
        "ùúá\n",
        "+\n",
        "ùúé\n",
        "Œº+œÉ).\n",
        "\n",
        "About 95% of the data falls within two standard deviations of the mean (\n",
        "ùúá\n",
        "‚àí\n",
        "2\n",
        "ùúé\n",
        "Œº‚àí2œÉ to\n",
        "ùúá\n",
        "+\n",
        "2\n",
        "ùúé\n",
        "Œº+2œÉ).\n",
        "\n",
        "About 99.7% of the data falls within three standard deviations of the mean (\n",
        "ùúá\n",
        "‚àí\n",
        "3\n",
        "ùúé\n",
        "Œº‚àí3œÉ to\n",
        "ùúá\n",
        "+\n",
        "3\n",
        "ùúé\n",
        "Œº+3œÉ).\n",
        "\n",
        "This is a key property that allows the normal distribution to be used in statistical inference.\n",
        "\n",
        "5. Asymptotic:\n",
        "The tails of the normal distribution curve approach, but never quite reach, the horizontal axis. In other words, the probability of extreme values decreases as you move further away from the mean, but never quite becomes zero.\n",
        "\n",
        "This means that theoretically, the distribution extends infinitely in both directions, but the probability of observing values far from the mean becomes extremely small.\n",
        "\n",
        "6. Mean and Standard Deviation Control the Shape:\n",
        "Mean (Œº): Shifts the entire distribution left or right. The mean determines the center of the distribution.\n",
        "\n",
        "Standard Deviation (œÉ): Controls the spread or dispersion of the data. A larger standard deviation results in a wider, flatter curve, while a smaller standard deviation results in a steeper, narrower curve.\n",
        "\n",
        "7. Probability Density Function (PDF):\n",
        "The probability density function for a normal distribution.\n",
        "\n",
        "\n",
        "ùúá\n",
        "Œº is the mean of the distribution.\n",
        "\n",
        "ùúé\n",
        "œÉ is the standard deviation.\n",
        "\n",
        "ùëí\n",
        "e is Euler‚Äôs number (approximately 2.71828).\n",
        "\n",
        "ùë•\n",
        "x is the variable or value for which you want to calculate the probability.\n",
        "\n",
        "8. The Standard Normal Distribution:\n",
        "A special case of the normal distribution is the standard normal distribution, which has a mean of 0 and a standard deviation of 1. It is commonly used for standardization and z-scores, where:\n",
        "\n",
        "The z-score represents the number of standard deviations an observation is away from the mean.\n",
        "\n",
        "9. Applications of Normal Distribution:\n",
        "Measurement Errors: The distribution of errors in measurements often follows a normal distribution, due to the Central Limit Theorem (CLT).\n",
        "\n",
        "Natural Phenomena: Many natural phenomena, such as human heights, weights, and IQ scores, tend to follow a normal distribution.\n",
        "\n",
        "Financial Markets: Stock returns and other financial data often exhibit normal or approximately normal behavior in the short run.\n",
        "\n",
        "Statistical Inference: The normal distribution plays a central role in hypothesis testing, confidence intervals, and many other inferential statistics methods due to the CLT."
      ],
      "metadata": {
        "id": "-ZHB2GIbr0qU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#12. What is the standard normal distribution, and why is it important?"
      ],
      "metadata": {
        "id": "FSKI6-Jhr0mC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The standard normal distribution is a special case of the normal distribution with a mean of 0 and a standard deviation of 1. It is often referred to as the Z-distribution and serves as a reference for comparing other normal distributions. The standard normal distribution is important because it enables standardization of data, meaning that values from any normal distribution can be transformed into z-scores to allow for comparisons on the same scale. This transformation simplifies calculations of probabilities and percentiles, which can then be easily accessed through standard normal tables or software. Additionally, the standard normal distribution plays a central role in the Central Limit Theorem, which asserts that the sum of many independent random variables will tend toward a normal distribution, regardless of the original distribution. It is also essential in hypothesis testing, particularly in z-tests, where it helps determine how likely sample data is under a given hypothesis. By converting data to z-scores, the standard normal distribution allows for consistent and simplified statistical analysis across different datasets."
      ],
      "metadata": {
        "id": "9Qrl_q6Er0ig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13. What is the Central Limit Theorem (CLT), and why is it critical in statistics?"
      ],
      "metadata": {
        "id": "fLmReqtsr0eq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Central Limit Theorem (CLT) is a fundamental concept in probability and statistics that states that the sampling distribution of the sample mean (or sum) will approach a normal distribution as the sample size increases, regardless of the shape of the original population distribution. This holds true as long as the samples are independent and come from the same population. The CLT is critical because it allows us to make statistical inferences about population parameters, even when the underlying population is not normally distributed."
      ],
      "metadata": {
        "id": "9YZg8ksKt8Hu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#14. How does the Central Limit Theorem relate to the normal distribution?"
      ],
      "metadata": {
        "id": "PNi2yPbxt8EB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Central Limit Theorem explains that, regardless of the shape of the original population distribution, the sampling distribution of the sample mean will tend toward a normal distribution as the sample size increases. This allows the normal distribution to be applied in many statistical analyses, even when the underlying data is not normally distributed. The CLT is what enables the powerful tools of hypothesis testing, confidence intervals, and probability calculations to be used effectively in statistics."
      ],
      "metadata": {
        "id": "2z8QXMPqt7_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#15. What is the application of Z statistics in hypothesis testing?"
      ],
      "metadata": {
        "id": "vSyPikI2t77R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Z statistics are widely used in hypothesis testing to determine whether a sample mean is significantly different from a population mean. They standardize the sample mean by converting it into a Z-score, allowing for comparisons using the normal distribution. Z-tests are particularly useful when the population standard deviation is known or when the sample size is large. They help assess the likelihood that an observed result is due to random chance, making them essential in making statistical inferences, constructing confidence intervals, and conducting tests like one-sample and two-sample z-tests."
      ],
      "metadata": {
        "id": "DmnesftMt73I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#16. How do you calculate a Z-score, and what does it represent?"
      ],
      "metadata": {
        "id": "thO0iyptt7zE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Z-score is a measure of how far a data point is from the mean, expressed in terms of standard deviations. It allows for comparison of values from different distributions or datasets, helps identify unusual or extreme values, and is foundational in statistical techniques like hypothesis testing and probability calculations. By calculating Z-scores, you can determine whether a data point is typical or rare within a given distribution."
      ],
      "metadata": {
        "id": "753sOsUct7rV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#17. What are point estimates and interval estimates in statistics?"
      ],
      "metadata": {
        "id": "en10C89Vt7mM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Point estimates provide a single value estimate for a population parameter, like the sample mean or proportion, offering no indication of uncertainty.\n",
        "\n",
        "* Interval estimates provide a range of possible values for the population parameter, with an associated confidence level that quantifies the degree of uncertainty.\n",
        "\n",
        "* Interval estimates are generally preferred because they give a more complete picture of the uncertainty associated with the estimate, which is essential for making more informed decisions based on data."
      ],
      "metadata": {
        "id": "OZMNAoewvme6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#18. What is the significance of confidence intervals in statistical analysis?"
      ],
      "metadata": {
        "id": "fr_XFVLZvvJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confidence intervals (CIs) are crucial in statistical analysis because they provide a range of plausible values for an unknown population parameter, such as the mean or proportion, based on sample data. Unlike point estimates, which provide a single value, confidence intervals offer a more comprehensive view by quantifying the uncertainty associated with the estimate. The width of the interval reflects the precision of the estimate, with narrower intervals indicating more precision and wider intervals suggesting greater uncertainty. Confidence intervals are used to assess statistical significance in hypothesis testing; for instance, if a confidence interval for the difference between two groups does not include zero, it suggests a statistically significant difference. Additionally, confidence intervals help in decision-making by providing a range of values within which the true parameter is likely to fall, allowing for more informed choices. They also provide the margin of error, giving insight into the possible range of error in the estimate. Ultimately, confidence intervals are a valuable tool in fields like medicine, marketing, economics, and polling, where they help to make predictions, inform policies, and guide decisions based on the variability in the data."
      ],
      "metadata": {
        "id": "Ebi-MtNav3Ms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#19. What is the relationship between a Z-score and a confidence interval?"
      ],
      "metadata": {
        "id": "Lw2xLJdDwJWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Z-score and confidence interval are closely related concepts in statistics. The Z-score is often used as a critical value when constructing a confidence interval, especially when the population standard deviation is known. It represents the number of standard deviations a data point (or sample statistic, like the sample mean) is from the population mean. When constructing a confidence interval, the Z-score defines the range around the sample mean where the true population parameter is likely to lie. For example, a 95% confidence interval uses a Z-score of approximately 1.96, meaning the interval will extend 1.96 standard deviations above and below the sample mean. The Z-score thus helps calculate the margin of error, determining the width of the confidence interval. Additionally, the Z-score corresponds to the confidence level, with higher confidence levels requiring larger Z-scores and wider intervals. In hypothesis testing, if the sample statistic falls outside the confidence interval defined by the Z-score, it suggests that the result is statistically significant. Ultimately, the Z-score quantifies the relationship between the sample statistic and the population parameter, making it essential for constructing and interpreting confidence intervals in statistical analysis."
      ],
      "metadata": {
        "id": "H4n5t7OJv3H8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#20. How are Z-scores used to compare different distributions?\n"
      ],
      "metadata": {
        "id": "laNWj_2cv3Dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Z-scores are used to compare different distributions by standardizing raw data points, making it possible to compare values across distributions with different means and standard deviations. By converting raw scores into Z-scores, you can assess the relative position of data points within each distribution, identify outliers, and compare probabilities or performance in a consistent way. Z-scores provide a common scale that allows comparisons to be made more meaningfully, regardless of the differences in the distributions themselves.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dIsj7y1mv2_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#21. What are the assumptions for applying the Central Limit Theorem?"
      ],
      "metadata": {
        "id": "ejK1A7AAv26L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Central Limit Theorem (CLT) relies on several key assumptions to ensure its validity. First, the observations in the sample must be independent, meaning that the outcome of one observation should not influence the outcome of another. Second, the sample size must be sufficiently large for the CLT to apply, with a common rule of thumb being that a sample size of at least 30 is needed, especially when the population distribution is not normal. Smaller sample sizes can be acceptable if the population distribution is already approximately normal. Third, the sample must be randomly selected to avoid bias, ensuring that every member of the population has an equal chance of being included. Additionally, the population must have a finite variance, meaning the data has a well-defined standard deviation. Finally, if the sample size is small, the underlying population distribution must be reasonably normal for the CLT to hold. When these assumptions are satisfied, the CLT guarantees that the sampling distribution of the sample mean will approximate a normal distribution as the sample size increases, enabling statistical inference even when the population distribution is not normal."
      ],
      "metadata": {
        "id": "G-jbL8j5v21a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#22. What is the concept of expected value in a probability distribution?"
      ],
      "metadata": {
        "id": "_Mu8hvBNv2wg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The expected value in a probability distribution is a key concept that represents the long-term average or mean of a random variable, taking into account the probabilities of its possible outcomes. It is essentially a weighted average, where each possible value of the random variable is multiplied by its probability of occurring. For discrete random variables, the expected value is calculated by summing the products of each outcome and its corresponding probability, while for continuous variables, it involves an integral over the probability density function. The expected value provides a measure of central tendency, indicating where the values of the random variable are likely to cluster in the long run, though it may not always correspond to an actual outcome. For example, when rolling a fair six-sided die, the expected value is 3.5, which isn't a possible outcome but represents the average result over many rolls. The expected value is widely used in decision-making, risk analysis, and various fields like economics and finance, as it helps assess the likely average outcome of random processes and supports informed choices under uncertainty."
      ],
      "metadata": {
        "id": "1lIv6OiSv2r4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#23.  How does a probability distribution relate to the expected outcome of a random variable."
      ],
      "metadata": {
        "id": "FPTScR3Fv2ne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A probability distribution describes the likelihood of all possible outcomes of a random variable, providing a framework for understanding the behavior of that variable. The expected outcome of a random variable, or expected value, is closely related to its probability distribution, as it represents the weighted average of all possible outcomes, with each outcome weighted by its probability.\n",
        "\n",
        "In essence, the expected value is derived from the probability distribution. It is calculated by summing (for discrete variables) or integrating (for continuous variables) the product of each possible outcome and its corresponding probability. The expected value tells us what the \"average\" or \"typical\" outcome is likely to be if the random process were repeated many times.\n",
        "\n",
        "For example, in the case of a discrete probability distribution such as a fair die roll, each face of the die has a probability of 1/6, and the expected value is the sum of each outcome multiplied by its probability, which in this case results in an average roll of 3.5. The probability distribution shows how likely each outcome is, and the expected value gives us a central point around which the outcomes are likely to cluster in the long run.\n",
        "\n",
        "Thus, the probability distribution provides the framework for all potential outcomes, and the expected value gives a summary statistic that reflects the center of that distribution. This relationship is essential for making predictions, decisions, and understanding the behavior of random variables in various fields, such as economics, finance, and statistics.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i0RDK_wHv2jg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#THANKYOU"
      ],
      "metadata": {
        "id": "8qzXRa2xv2fh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K9iEqyDOzCBi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}